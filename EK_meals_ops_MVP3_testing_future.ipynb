{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>550</td><td>application_1532233567143_0320</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-ge-spa.ympkagzigm5elfiwak4c3kqbbb.fx.internal.cloudapp.net:8088/proxy/application_1532233567143_0320/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-ge-spa.ympkagzigm5elfiwak4c3kqbbb.fx.internal.cloudapp.net:30060/node/containerlogs/container_e11_1532233567143_0320_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "## Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from collections import Counter\n",
    "from numpy import loadtxt\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create Spark Session\n",
    "\n",
    "spark = SparkSession.builder.appName(\"First SparkSession\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extract pax and flight data from db table: spark_pax_flight_future_final\n",
    "\n",
    "spark_future_pax_data = spark.sql(\"select * from spark_pax_flight_future_final_for_pyspark_test \")   \n",
    "#print(type(spark_future_pax_data)): spark_future_pax_data is a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename Columns in spark_future_pax_data\n",
    "\n",
    "spark_future_pax_data = spark_future_pax_data.withColumnRenamed(\"board_point\", \"flight_boarding_pt\")\n",
    "spark_future_pax_data = spark_future_pax_data.withColumnRenamed(\"menu_name\", \"menuname\")\n",
    "\n",
    "#print(spark_future_pax_data.describe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dishsubcategory column has empty strings, replace those values with \"Poultry\"\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark_future_pax_data = spark_future_pax_data.withColumn(\"dishsubcategory\", \\\n",
    "              when(spark_future_pax_data[\"menucardname\"] == \"Cajun chicken\", \"Poultry\").otherwise(spark_future_pax_data[\"dishsubcategory\"]))\n",
    "\n",
    "#spark_future_pax_data.select('dishsubcategory').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Selecting all Hot Meals from the Main Course into M1 dataframe\n",
    "\n",
    "M1 = spark_future_pax_data.where((col('meal_service_name') == 'Hot Meal') & (col('dishcategory') == 'Main Course')) \n",
    "#M1.count()\n",
    "#M1.show()\n",
    "\n",
    "#M1.select('dishsubcategory').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Cuisine data\n",
    "\n",
    "cuisine = spark.sql(\"select trim(lower(itemname)) as itemname,trim(lower(cuisine)) as cuisine from  dish_cuisine_sandra_from_banus_local_for_pyspark_test \")  \n",
    "cuisine = cuisine.drop_duplicates()\n",
    "cuisine = cuisine.withColumn(\"Cuisine\", regexp_replace('cuisine', '\\?', ''))\n",
    "#cuisine = cuisine.withColumn(\"itemname\", regexp_replace('itemname', '\\?', ''))\n",
    "#cuisine.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuisine count\n",
      "241"
     ]
    }
   ],
   "source": [
    "print(\"cuisine count\")\n",
    "cuisine.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['itemname', 'cuisine']\n",
    "vals = [\n",
    "    ('beef ragoût','continental / european'),\n",
    "    ('boiled beef with apple and horseradish purée','german'),\n",
    "    ('grilled beef fillet with béarnaise sauce','continental / european'),\n",
    "    ('lamb ragoût','continental / european'),\n",
    "    ('seafood with chive velouté','continental / european'),\n",
    "    ('seafood á l\\'armoricaine','continental / european'),\n",
    "    ('seafood à l\\'armoricaine','continental / european'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "cuisine_specialCharacters = spark.createDataFrame(vals, columns)\n",
    "cuisine_wspecialCharacters = cuisine.union(cuisine_specialCharacters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M1 = M1.withColumn(\"menucardname\", lower(col(\"menucardname\")))\n",
    "M1 = M1.withColumn(\"menucardname\", trim(M1.menucardname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 before joining with cuisine\n",
      "49728"
     ]
    }
   ],
   "source": [
    "print(\"M1 before joining with cuisine\")\n",
    "M1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Join M1 and Cuisine dataframes \n",
    "\n",
    "M1 = M1.join(cuisine_wspecialCharacters, M1.menucardname==cuisine.itemname, how='left')\n",
    "#M1.select('menuname').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 after joining with cuisine\n",
      "49728"
     ]
    }
   ],
   "source": [
    "print(\"M1 after joining with cuisine\")\n",
    "M1.count() #-- 49728\n",
    "#cuisine.count() -239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "M1 = M1.withColumn(\"menuname\", \\\n",
    "                    when(M1[\"menuname\"] == \"F DXBAUS HM J Q A\", \"DXBAUS HM J Q A\").otherwise(M1.menuname))\n",
    "\n",
    "M1 = M1.withColumn(\"menuname\", \\\n",
    "                   when(M1[\"menuname\"] == \"FEST2017 DXBEUR HMJ\",\"DXBEUR HMJ FEST2017\").otherwise(M1.menuname))\n",
    "\n",
    "M1 = M1.withColumn(\"menuname\", \\\n",
    "                   when(M1[\"menuname\"] == \"FEST2017 DXBGER HMJ\",\"DXBGER HMJ FEST2017\").otherwise(M1.menuname))\n",
    "\n",
    "M1 = M1.withColumn(\"menuname\", \\\n",
    "                   when(M1[\"menuname\"] == \"HO 2017 DXBCDG HM JB\",\"DXBCDG HM JB\").otherwise(M1.menuname))\n",
    "\n",
    "M1 = M1.withColumn(\"menuname\", \\\n",
    "                   when(M1[\"menuname\"] == \"TR DXBMEL HM J T2\", \"DXBMEL HM J T\").otherwise(M1.menuname))\n",
    "\n",
    "#M1.select('menuname').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "M1 = M1.withColumn('menu_cycle', split(reverse(M1.menuname), ' ')[0])\n",
    "\n",
    "#M1.select('menu_cycle').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|    dishsubcategory|\n",
      "+-------------------+\n",
      "|            Poultry|\n",
      "|           Red Meat|\n",
      "|            Seafood|\n",
      "|Pasta or Vegetarian|\n",
      "+-------------------+"
     ]
    }
   ],
   "source": [
    "M1.select('dishsubcategory').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "M1 = M1.withColumn(\"menu_cycle\", \\\n",
    "                   when(M1[\"menu_cycle\"] == \"JA\", \"A\").otherwise(M1.menu_cycle))\n",
    "M1 = M1.withColumn(\"menu_cycle\", \\\n",
    "                   when(M1[\"menu_cycle\"] == \"JB\", \"B\").otherwise(M1.menu_cycle))\n",
    "M1 = M1.withColumn(\"menu_cycle\", \\\n",
    "                   when(M1[\"menu_cycle\"] == \"FEST17\", \"FEST2017\").otherwise(M1.menu_cycle))\n",
    "\n",
    "M1 = M1.withColumn('destination', substring(split(M1.menuname, ' ')[0],4,3))\n",
    "\n",
    "#M1.select('destination','menuname').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pax ages to be 'bin'ed\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "M1 = M1.withColumn('date_of_birth',to_date(M1.date_of_birth))\n",
    "M1 = M1.withColumn('today_date',current_date())\n",
    "#M1 = M1.withColumn('age1',floor(datediff(M1.today_date,M1.date_of_birth)/365).cast(DoubleType()))\n",
    "#M1 = M1.withColumn('age',(datediff(M1.today_date,M1.date_of_birth)/365).cast(DoubleType()))\n",
    "M1 = M1.withColumn('age',(datediff(M1.today_date,M1.date_of_birth)/365.2425).cast(DoubleType()))\n",
    "\n",
    "#M1.select('age').distinct().show()\n",
    "\n",
    "M1=M1.withColumn('age',when(M1.age.isNull(),-1).otherwise(M1.age))\n",
    "\n",
    "bucketizer = Bucketizer(splits=[float(\"-inf\"),0, 12, 19, 40, 60,100, float(\"inf\")],inputCol=\"age\", outputCol=\"age_group\")\n",
    "M1 = bucketizer.setHandleInvalid(\"keep\").transform(M1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----+\n",
      "|age_group|age_group_1|count|\n",
      "+---------+-----------+-----+\n",
      "|      5.0|     Elders| 7941|\n",
      "|      3.0|     Adults|12379|\n",
      "|      1.0|   Children| 2307|\n",
      "|      4.0|Middle Aged|21057|\n",
      "|      2.0|  Teenagers| 1690|\n",
      "|      0.0|    Unknown| 4354|\n",
      "+---------+-----------+-----+"
     ]
    }
   ],
   "source": [
    "t = {0.0:\"Unknown\",\n",
    "     1.0:\"Children\", \n",
    "     2.0:\"Teenagers\", \n",
    "     3.0:\"Adults\", \n",
    "     4.0:\"Middle Aged\", \n",
    "     5.0:\"Elders\",\n",
    "     6.0:\"Unknown_agegroup\"}\n",
    "bucket_name_ = udf(lambda x: t[x], StringType())\n",
    "M1 = M1.withColumn(\"age_group_1\", bucket_name_(\"age_group\"))\n",
    "M1.groupby([M1.age_group,M1.age_group_1]).count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Country data\n",
    "##!!!!!!!!!!!!!!!!!!!!!!!!! disable\n",
    "M1=M1.withColumn('nationality',psf.when(M1.nationality == 'NA',\"\").otherwise(M1.nationality))\n",
    "country_codes = spark.sql(\"select `alpha-2` as alpha_2,country_region as country_region from country_codes_cuisine\")   #no nulls in columns, only empty strings\n",
    "\n",
    "M1 = M1.join(country_codes, M1.nationality==country_codes.alpha_2, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 after joining with country codes\n",
      "49728"
     ]
    }
   ],
   "source": [
    "print(\"M1 after joining with country codes\")\n",
    "M1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform flight level aggregations - dishsubcategory\n",
    "\n",
    "#drop dups\n",
    "temp1_dishsub_cuisine = M1.select('flight_number', 'flight_boarding_pt','flight_boarding_time','dishsubcategory','Cuisine').drop_duplicates()\n",
    "\n",
    "#the only column with NaNs is Cuisine. Replace those with 'Unknown' as the one-hot code in pyspark doesn't allow usage of columns with Nan\n",
    "#temp1_dishsub_cuisine.where(col(\"Cuisine\").isNull()).count() -- 530\n",
    "temp1_dishsub_cuisine=temp1_dishsub_cuisine.na.fill({'Cuisine':'Unknown Cuisine'})\n",
    " \n",
    "# below function to create as many values in the list as the distinct values in 'Cuisine' and 'dishsubcategory'\n",
    "cuisine_categories = temp1_dishsub_cuisine.select('Cuisine').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "dishsubcatgories = temp1_dishsub_cuisine.select('dishsubcategory').distinct().rdd.flatMap(lambda x : x).collect()\n",
    "cuisine_categories.sort()\n",
    "dishsubcatgories.sort()\n",
    "\n",
    "# one-hot encoding \n",
    "for category in dishsubcatgories:\n",
    "    function = udf(lambda item: 1 if item == category else 0, IntegerType())\n",
    "    new_column_name = 'dishsubcategory'+'_'+category\n",
    "    temp1_dishsub_cuisine = temp1_dishsub_cuisine.withColumn(new_column_name, function(col('dishsubcategory')))\n",
    "\n",
    "for category in cuisine_categories:\n",
    "    function = udf(lambda item: 1 if item == category else 0, IntegerType())\n",
    "    new_column_name = 'Cuisine'+'_'+category\n",
    "    temp1_dishsub_cuisine = temp1_dishsub_cuisine.withColumn(new_column_name, function(col('Cuisine')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pasta or Vegetarian', 'Poultry', 'Red Meat', 'Seafood']"
     ]
    }
   ],
   "source": [
    "dishsubcatgories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp1_dishsub_cuisine after one hot for dishsubcategory and cuisine\n",
      "678"
     ]
    }
   ],
   "source": [
    "print(\"temp1_dishsub_cuisine after one hot for dishsubcategory and cuisine\")\n",
    "temp1_dishsub_cuisine.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight_number', 'flight_boarding_pt', 'flight_boarding_time', 'dishsubcategory_Pasta or Vegetarian', 'dishsubcategory_Poultry', 'dishsubcategory_Red Meat', 'dishsubcategory_Seafood', 'Cuisine_Unknown Cuisine', 'Cuisine_asian', 'Cuisine_asian / japanese', 'Cuisine_continental / european', 'Cuisine_indian', 'Cuisine_middle eastern', 'Cuisine_middle eastern / gulf']"
     ]
    }
   ],
   "source": [
    "# dropping 'Cuisine', 'dishsubcategory' columns\n",
    "drop_list = ['Cuisine', 'dishsubcategory']\n",
    "\n",
    "temp1_dishsub_cuisine=temp1_dishsub_cuisine.select([column for column in temp1_dishsub_cuisine.columns if column not in drop_list])\n",
    "temp1_dishsub_cuisine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform Flight level aggregations - demographics\n",
    "\n",
    "# columns to be grouped\n",
    "group_by_cols = ['flight_number','flight_boarding_pt','flight_boarding_time','destination','menu_cycle','service_category_code']\n",
    "\n",
    "# columns to be pivoted\n",
    "pivot_cols = ['age_group_1','gender','country_region']\n",
    "\n",
    "# create a static/constant col to perform sum\n",
    "M1 = M1.withColumn(\"values\", lit(1))\n",
    "\n",
    "# drop dups and store in temp1 dataframe\n",
    "temp1 = M1[['flight_number','flight_boarding_pt','flight_boarding_time','pax_id','menu_cycle','destination','service_category_code',\n",
    "            'age_group_1','gender', 'country_region', 'values']].drop_duplicates()\n",
    "\n",
    "#temp1.count() -- 16223\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pivot on 'age_group_1' column\n",
    "temp1_demographics  = temp1.groupby(group_by_cols).pivot(pivot_cols[0]).agg(sum(temp1.values))\n",
    "\n",
    "#pivot on 'gender' column\n",
    "tempdf2 = temp1.groupby(group_by_cols).pivot(pivot_cols[1]).agg(sum(temp1.values))\n",
    "\n",
    "#pivot on 'country_region' column , need to drop NaNs or replace them, else group by doesn't work\n",
    "temp1 = temp1.fillna({'country_region':'Unknown country region'})\n",
    "\n",
    "tempdf3 = temp1.groupby(group_by_cols).pivot('country_region').agg(sum(temp1.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge all 3 pivot tables\n",
    "\n",
    "tempdf2 = tempdf2.join(tempdf3, on=group_by_cols, how='inner')\n",
    "\n",
    "temp1_demographics=temp1_demographics.join(tempdf2, on = group_by_cols, how='inner')\n",
    "\n",
    "#temp1_demographics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp1_demographics after one hot for dishsubcategory and cuisine\n",
      "224"
     ]
    }
   ],
   "source": [
    "print(\"temp1_demographics after one hot for dishsubcategory and cuisine\")\n",
    "temp1_demographics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge demographics and dishsubcuisine dfs\n",
    "\n",
    "temp1 = temp1_demographics.join(temp1_dishsub_cuisine,  on = ['flight_number', 'flight_boarding_pt', \n",
    "                               'flight_boarding_time'] , how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp1 after merging temp1_demographics and temp_dishsubcuisine\n",
      "678"
     ]
    }
   ],
   "source": [
    "print(\"temp1 after merging temp1_demographics and temp_dishsubcuisine\")\n",
    "temp1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# flight_boarding_time is a string, convert to timestamp and extract date components\n",
    "\n",
    "temp1 = temp1.withColumn('flight_boarding_time_conv',\n",
    "                         unix_timestamp(temp1['flight_boarding_time'],'yyyyMMddHHmm').cast(\"timestamp\"))\n",
    "\n",
    "temp1 = temp1.withColumn(\"year\",year(\"flight_boarding_time_conv\"))\n",
    "\n",
    "temp1 = temp1.withColumn(\"month\",month(\"flight_boarding_time_conv\"))\n",
    "                         \n",
    "temp1 = temp1.withColumn(\"quarter\",quarter(\"flight_boarding_time_conv\"))\n",
    "                         \n",
    "temp1 = temp1.withColumn(\"week\",weekofyear(\"flight_boarding_time_conv\"))\n",
    "\n",
    "temp1 = temp1.withColumn(\"day\",dayofmonth(\"flight_boarding_time_conv\"))\n",
    "\n",
    "temp1 = temp1.withColumn('dayofweek',(date_format(temp1.flight_boarding_time_conv, 'u')-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#date_format(temp1.flight_boarding_time, 'u') \n",
    "\n",
    "#temp1.select(dayofweek('day').alias('dayweek')).collect()\n",
    "#df.select(dayofweek('dt').alias('day')).collect()\n",
    "#temp1.select(date_trunc('year', temp1.flight_boarding_time).alias('year')).collect()\n",
    "#temp1.select(date_trunc('day', temp1.flight_boarding_time).alias('dow')).collect()\n",
    "\n",
    "#temp1.select(dayofmonth('flight_boarding_time').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#temp1.select('flight_boarding_time','year','month','quarter','week','day').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify column names\n",
    " \n",
    "new_list1= list(map(lambda x: x.replace(\"Cuisine_\", \"\"), temp1.columns))\n",
    "\n",
    "temp1 = temp1.toDF(*new_list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify column names\n",
    "\n",
    "new_list2= list(map(lambda x: x.replace(\"dishsubcategory_\", \"\"), temp1.columns))\n",
    "\n",
    "temp1 = temp1.toDF(*new_list2)\n",
    "\n",
    "#temp1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight_number', 'flight_boarding_pt', 'flight_boarding_time', 'destination', 'menu_cycle', 'service_category_code', 'Adults', 'Children', 'Elders', 'Middle Aged', 'Teenagers', 'Unknown', 'C', 'F', 'M', '', 'Africa', 'Americas', 'Asia', 'China', 'Europe', 'Germany', 'Gulf', 'India', 'Indonesia', 'Iran', 'Italy', 'Japan', 'Mauritus', 'Northern Africa', 'Oceania', 'Thailand', 'Unknown country region', 'Meal_Pasta or Vegetarian', 'Meal_Poultry', 'Meal_Red Meat', 'Meal_Seafood', 'Unknown Cuisine', 'asian', 'asian / japanese', 'continental / european', 'indian', 'middle eastern', 'middle eastern / gulf', 'flight_boarding_time_conv', 'year', 'month', 'quarter', 'week', 'day', 'dayofweek']"
     ]
    }
   ],
   "source": [
    "# Rename certain Columns\n",
    "\n",
    "temp1 = temp1.withColumnRenamed(\"Poultry\", \"Meal_Poultry\")\n",
    "temp1 = temp1.withColumnRenamed(\"Red Meat\", \"Meal_Red Meat\")  \n",
    "temp1 = temp1.withColumnRenamed(\"Seafood\", \"Meal_Seafood\")\n",
    "temp1 = temp1.withColumnRenamed(\"Pasta or Vegetarian\", \"Meal_Pasta or Vegetarian\")\n",
    "\n",
    "temp1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678"
     ]
    }
   ],
   "source": [
    "temp1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+--------------------+------------+-------------+------------+------------------------+\n",
      "|flight_number|flight_boarding_pt|flight_boarding_time|Meal_Poultry|Meal_Red Meat|Meal_Seafood|Meal_Pasta or Vegetarian|\n",
      "+-------------+------------------+--------------------+------------+-------------+------------+------------------------+\n",
      "|         0073|               DXB|        201806260820|           0|            0|           1|                       0|\n",
      "|         0073|               DXB|        201806260820|           1|            0|           0|                       0|\n",
      "|         0073|               DXB|        201806260820|           0|            1|           0|                       0|\n",
      "|         0031|               DXB|        201806241125|           1|            0|           0|                       0|\n",
      "|         0031|               DXB|        201806241125|           0|            0|           1|                       0|\n",
      "|         0031|               DXB|        201806241125|           0|            1|           0|                       0|\n",
      "|         0081|               DXB|        201806251435|           0|            0|           1|                       0|\n",
      "|         0081|               DXB|        201806251435|           0|            1|           0|                       0|\n",
      "|         0081|               DXB|        201806251435|           1|            0|           0|                       0|\n",
      "+-------------+------------------+--------------------+------------+-------------+------------+------------------------+\n",
      "only showing top 9 rows"
     ]
    }
   ],
   "source": [
    "temp1.select('flight_number','flight_boarding_pt', 'flight_boarding_time','Meal_Poultry','Meal_Red Meat','Meal_Seafood','Meal_Pasta or Vegetarian').show(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#temp1.where(col(\"Cuisine\").isNull()).count() -- 530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_melt = temp1.columns\n",
    "list_melt = [e for e in list_melt if e not in ('Meal_Poultry', 'Meal_Red Meat', 'Meal_Seafood','Meal_Pasta or Vegetarian')]\n",
    "#list_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Meal_Pasta or Vegetarian', 'Meal_Poultry', 'Meal_Red Meat', 'Meal_Seafood']"
     ]
    }
   ],
   "source": [
    "melt_cols= temp1.columns\n",
    "melt_cols = [e for e in melt_cols if e in ('Meal_Poultry', 'Meal_Red Meat', 'Meal_Seafood','Meal_Pasta or Vegetarian')]\n",
    "melt_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "\n",
    "def melt_df(\n",
    "        df: DataFrame,\n",
    "        id_vars: Iterable[str],# columns to remain as is\n",
    "        value_vars: Iterable[str], # columns to convert to rows\n",
    "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
    "    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n",
    "\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = array(*(\n",
    "        struct(lit(c).alias(var_name), col(c).alias(value_name))\n",
    "        for c in value_vars))\n",
    "\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n",
    "\n",
    "    cols = id_vars + [\n",
    "            col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = melt_df(temp1, list_melt, melt_cols, 'variable', 'Meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 after melting temp1\n",
      "2712"
     ]
    }
   ],
   "source": [
    "print(\"df1 after melting temp1\")\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----+\n",
      "|flight_number|            variable|Meal|\n",
      "+-------------+--------------------+----+\n",
      "|         0073|Meal_Pasta or Veg...|   0|\n",
      "|         0073|        Meal_Poultry|   0|\n",
      "|         0073|       Meal_Red Meat|   0|\n",
      "|         0073|        Meal_Seafood|   1|\n",
      "|         0073|Meal_Pasta or Veg...|   0|\n",
      "|         0073|        Meal_Poultry|   1|\n",
      "|         0073|       Meal_Red Meat|   0|\n",
      "|         0073|        Meal_Seafood|   0|\n",
      "|         0073|Meal_Pasta or Veg...|   0|\n",
      "|         0073|        Meal_Poultry|   0|\n",
      "|         0073|       Meal_Red Meat|   1|\n",
      "|         0073|        Meal_Seafood|   0|\n",
      "|         0031|Meal_Pasta or Veg...|   0|\n",
      "|         0031|        Meal_Poultry|   0|\n",
      "|         0031|       Meal_Red Meat|   0|\n",
      "|         0031|        Meal_Seafood|   1|\n",
      "|         0031|Meal_Pasta or Veg...|   0|\n",
      "|         0031|        Meal_Poultry|   0|\n",
      "|         0031|       Meal_Red Meat|   1|\n",
      "|         0031|        Meal_Seafood|   0|\n",
      "|         0031|Meal_Pasta or Veg...|   0|\n",
      "|         0031|        Meal_Poultry|   1|\n",
      "|         0031|       Meal_Red Meat|   0|\n",
      "|         0031|        Meal_Seafood|   0|\n",
      "|         0081|Meal_Pasta or Veg...|   0|\n",
      "+-------------+--------------------+----+\n",
      "only showing top 25 rows"
     ]
    }
   ],
   "source": [
    "df1.select('flight_number','variable','Meal').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_col = split(df1['variable'], '_')\n",
    "df1 = df1.withColumn('tmp', split_col.getItem(0))\n",
    "df1 = df1.withColumn('cat', split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop_list = ['tmp', 'variable']\n",
    "\n",
    "df1=df1.select([column for column in df1.columns if column not in drop_list])\n",
    "\n",
    "df1=df1.orderBy('flight_number', 'flight_boarding_pt', 'flight_boarding_time')\n",
    "\n",
    "df1 = df1[df1.Meal == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#M1.select('flight_number','flight_boarding_pt', 'flight_boarding_time','pax_id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pax_count = M1[['flight_number','flight_boarding_pt', 'flight_boarding_time','pax_id']].groupby(['flight_number', 'flight_boarding_pt', \n",
    "                           'flight_boarding_time']).agg(countDistinct('pax_id').alias(\"pax_count\"))\n",
    "\n",
    "\n",
    "#pax_count.count() -- 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pax_count\n",
      "224"
     ]
    }
   ],
   "source": [
    "print(\"pax_count\")\n",
    "pax_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df1 = df1.join(pax_count,  on = ['flight_number', 'flight_boarding_pt', \n",
    "                               'flight_boarding_time'],how='left')\n",
    "\n",
    "\n",
    "df1 = df1.withColumnRenamed(\"cat\", \"dishsubcategory\")\n",
    "df1 = df1.withColumnRenamed(\"service_category_code\", \"itemcategory\")  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 count after joining df1 with pax_count\n",
      "678"
     ]
    }
   ],
   "source": [
    "print(\"df1 count after joining df1 with pax_count\")\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df1.withColumn(\"itemcategory\", \\\n",
    "                   when(df1[\"itemcategory\"] == \"L\", \"Lunch\").otherwise(df1[\"itemcategory\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df1.withColumn(\"itemcategory\", \\\n",
    "                   when(df1[\"itemcategory\"] == \"D\", \"Dinner\").otherwise(df1[\"itemcategory\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df1.withColumnRenamed(\"\", \"Unknown column\")\n",
    "#df1.select('Unknown column').distinct().show()\n",
    "\n",
    "#M1.select('dishsubcategory').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('flight_boarding_time',unix_timestamp(temp1['flight_boarding_time'],'yyyyMMddHHmm').cast(\"timestamp\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop_list=['flight_boarding_time_conv','Unknown','Unknown column','Unknown country region','Unknown Cuisine']\n",
    "df1=df1.select([column for column in df1.columns if column not in drop_list])\n",
    "#df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"mytempTable\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'`default`.`ek_meals_ops_future_test_pyspark_test` already exists.;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/context.py\", line 384, in sql\n",
      "    return self.sparkSession.sql(sqlQuery)\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 545, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
      "  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: '`default`.`ek_meals_ops_future_test_pyspark_test` already exists.;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"create table ek_meals_ops_future_test_pyspark_test as select * from mytempTable\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df1.select('pax_count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678"
     ]
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SELECT MY.FLIGHT_NUMBER, MY.FLIGHT_BOARDING_PT,MY.FLIGHT_BOARDING_TIME,MY.ITEMCATEGORY,MY.DISHSUBCATEGORY,MY.MENU_CYCLE,MY.DESTINATION,\n",
    "my.`flight_number` - p.`flight_number` as `flight_number_diff`,\n",
    "my.`flight_boarding_pt` - p.`flight_boarding_pt` as `flight_boarding_pt_diff`,\n",
    "my.`flight_boarding_time` - p.`flight_boarding_time` as `flight_boarding_time_diff`,\n",
    "my.`destination` - p.`destination` as `destination_diff`,\n",
    "my.`menu_cycle` - p.`menu_cycle` as `menu_cycle_diff`,\n",
    "my.`itemcategory` - p.`itemcategory` as `itemcategory_diff`,\n",
    "my.`adults` - p.`adults` as `adults_diff`,\n",
    "my.`children` - p.`children` as `children_diff`,\n",
    "my.`elders` - p.`elders` as `elders_diff`,\n",
    "my.`middle aged` - p.`middle aged` as `middle aged_diff`,\n",
    "my.`teenagers` - p.`teenagers` as `teenagers_diff`,\n",
    "my.`c` - p.`c` as `c_diff`,\n",
    "my.`f` - p.`f` as `f_diff`,\n",
    "my.`m` - p.`m` as `m_diff`,\n",
    "my.`africa` - p.`africa` as `africa_diff`,\n",
    "my.`americas` - p.`americas` as `americas_diff`,\n",
    "my.`asia` - p.`asia` as `asia_diff`,\n",
    "my.`china` - p.`china` as `china_diff`,\n",
    "my.`europe` - p.`europe` as `europe_diff`,\n",
    "my.`germany` - p.`germany` as `germany_diff`,\n",
    "my.`gulf` - p.`gulf` as `gulf_diff`,\n",
    "my.`india` - p.`india` as `india_diff`,\n",
    "my.`indonesia` - p.`indonesia` as `indonesia_diff`,\n",
    "my.`iran` - p.`iran` as `iran_diff`,\n",
    "my.`italy` - p.`italy` as `italy_diff`,\n",
    "my.`japan` - p.`japan` as `japan_diff`,\n",
    "my.`mauritus` - p.`mauritus` as `mauritus_diff`,\n",
    "my.`northern africa` - p.`northern africa` as `northern africa_diff`,\n",
    "my.`oceania` - p.`oceania` as `oceania_diff`,\n",
    "my.`thailand` - p.`thailand` as `thailand_diff`,\n",
    "my.`asian` - p.`asian` as `asian_diff`,\n",
    "my.`asian / japanese` - p.`asian / japanese` as `asian / japanese_diff`,\n",
    "my.`continental / european` - p.`continental / european` as `continental / european_diff`,\n",
    "my.`indian` - p.`indian` as `indian_diff`,\n",
    "my.`middle eastern` - p.`middle eastern` as `middle eastern_diff`,\n",
    "my.`middle eastern / gulf` - p.`middle eastern / gulf` as `middle eastern / gulf_diff`,\n",
    "my.`year` - p.`year` as `year_diff`,\n",
    "my.`month` - p.`month` as `month_diff`,\n",
    "my.`quarter` - p.`quarter` as `quarter_diff`,\n",
    "my.`week` - p.`week` as `week_diff`,\n",
    "my.`day` - p.`day` as `day_diff`,\n",
    "my.`dayofweek` - p.`dayofweek` as `dayofweek_diff`,\n",
    "my.`meal` - p.`meal` as `meal_diff`,\n",
    "my.`dishsubcategory` - p.`dishsubcategory` as `dishsubcategory_diff`,\n",
    "my.`pax_count` - p.`pax_count` as `pax_count_diff`\n",
    " FROM ek_meals_ops_future_test_pyspark_test MY, ek_meals_ops_future_test_pyspark_test_prudvi P \n",
    " WHERE (MY.FLIGHT_NUMBER=P.FLIGHT_NUMBER \n",
    "\t\tAND MY.FLIGHT_BOARDING_TIME=P.FLIGHT_BOARDING_TIME \n",
    "\t\tAND MY.FLIGHT_BOARDING_PT=P.FLIGHT_BOARDING_PT \n",
    "\t\tAND MY.ITEMCATEGORY=P.ITEMCATEGORY  \n",
    "\t\tAND MY.DISHSUBCATEGORY=P.DISHSUBCATEGORY  \n",
    "\t\tAND MY.MENU_CYCLE=P.MENU_CYCLE  \n",
    "\t\tAND MY.DESTINATION=P.DESTINATION )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
