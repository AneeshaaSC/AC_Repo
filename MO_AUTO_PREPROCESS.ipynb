{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Meals Operation Automation Preprocessing for EK\n",
    "### Combining pax and flight data set\n",
    "### Source Data set: \n",
    "###    Bringing the dataset for flights\n",
    "###    macs_flight_mvp3 where operation_date >= RUN_DATE union h_macs_flight_full_new \n",
    "###    Bringing the dataset for pax\n",
    "###    h_macs_pax_joined_all_new where where boarding_date >= '2016-12-12 00:00:00.0' and upper(travelled_class) = 'J' UNION\n",
    "###    macs_pax_joined_mvp3 where boarding_date >= RUN_DATE\n",
    "### Target Dataset: spark_pax_flight_final\n",
    "### Running Instruction: Define variable RUN_DATE and execute the entire Code in YYYYMMDDHH24MISS\n",
    "###                      loadtype = 'fullload' for first time load\n",
    "###                      loadtype = 'incrementalload' for second time load onwards\n",
    "### The data before and equal RUN_DATE will be treated as History Data and after RUN_DATE + 1 would be treated as Future data\n",
    "\n",
    "### Possible execution time of this code is: fullload time taken: 40 mins & 51 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Importing required python package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json          \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "### This code is for POC purpose. Dont deploy this in production environment without tuning further and \n",
    "### implement exception handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2018-07-27 09:28:04'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "from time import gmtime, strftime\n",
    "strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define Increment Load or Full Load\n",
    "## Precautions for full load. Manually drop spark_pax_flight_final hive table before running this process.\n",
    "loadtype = 'incrementalload'\n",
    "#loadtype = 'fullload'\n",
    "#RUN_DATE = '20180620000000' #today - 6 days\n",
    "RUN_DATE = '20180731000000' #today - 6 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire load will happen upto 2018-05-31 00:00:00\n",
      "and this is incrementalload"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# naive datetime\n",
    "rundate = datetime.datetime.strptime(RUN_DATE, '%Y%m%d%H%M%S')\n",
    "\n",
    "print(\"The entire load will happen upto {}\".format(rundate))\n",
    "print(\"and this is {}\".format(loadtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Create spark session \n",
    "### This steps required to run from jupyter\n",
    "spark = SparkSession.builder.appName(\"First SparkSession\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\" CREATE TABLE IF NOT EXISTS loadtable ( rundate string ) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"INSERT INTO TABLE loadtable SELECT {0} \".format(RUN_DATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Bringing the dataset for flights\n",
    "spark_flight = spark.sql(\"select id, airline_designator, flight_number, CONCAT(from_unixtime(UNIX_TIMESTAMP(operation_date, 'yyyy-MM-dd'), 'yyyyMMdd' ), operation_time) as flight_boarding_time , board_point, aircraft_type, class_configuration, pantry_code, crew_code, gate_number, baggage_concept, flight_status from macs_flight_mvp3 union all select  id, airline_designator, flight_number, CONCAT(from_unixtime(UNIX_TIMESTAMP(operation_date, 'yyyy-MM-dd'), 'yyyyMMdd' ), operation_time) as flight_boarding_time , board_point, aircraft_type, class_configuration, pantry_code, crew_code, gate_number, baggage_concept, flight_status from h_macs_flight_full_new \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spark_flight.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### testing spark_flight data \n",
    "# print(spark_flight.count()) ### Testing result 1884964 as of 6/19\n",
    "#spark_flight.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## spark_flight_new = spark_flight.select(\"flight_number\" , \"flight_boarding_time\" , \"board_point\" ).unique()\n",
    "\n",
    "spark_flight_new = spark_flight.drop_duplicates([ \"flight_number\" , \"flight_boarding_time\" , \"board_point\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### testing spark_flight data \n",
    "# print(spark_flight_new.count()) ### Testing result 1884964 as of 6/19  8936516\n",
    "# spark_flight.show()\n",
    "# type(spark_flight_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Bringing the dataset for pax\n",
    "spark_pax = spark.sql(\"select pax_id, pax_booking_status, destination, cabin_class, transit_indicator, booked_class, travelled_class, seat_number as SeatNumber, pax_type, pax_group_code, boarding_date, upgrade_indicator, ticket_class, handicapped, unaccompanied_minor, priority_pax, check_in_city, date_of_birth, nationality, gender, country_of_birth, country_of_residence, mfl_id as id from h_macs_pax_joined_all_new where boarding_date >= '2016-12-12 00:00:00.0' and upper(travelled_class) = 'J' UNION select pax_id, pax_booking_status, destination, cabin_class, transit_indicator, booked_class, travelled_class, seat_number as SeatNumber, pax_type, pax_group_code, boarding_date, upgrade_indicator, ticket_class, handicapped, unaccompanied_minor, priority_pax, check_in_city, date_of_birth, nationality, gender, country_of_birth, country_of_residence, mfl_id as id from macs_pax_joined_mvp3 where ( cabin_class = 'J' or travelled_class = 'J' ) \") \n",
    "\n",
    "# and operation_date > ( select from_unixtime(unix_timestamp(max(rundate),'yyyyMMddHHmmss' )) from loadtable ) \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### testing spark_pax data \n",
    "# print(spark_pax.count())  ## 20667814  8936516\n",
    "# spark_pax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark_pax_flight_key  = [ \"id\" ]\n",
    "spark_pax_flight = spark_flight_new.join(spark_pax , spark_pax_flight_key , \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### testing spark_pax_flight data \n",
    "### print(spark_pax_flight.count())\n",
    "#spark_pax_flight.show()\n",
    "print(loadtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Loading the pax and flights data into Hive table\n",
    "spark_pax_flight.createOrReplaceTempView(\"spark_pax_flight\")\n",
    "\n",
    "if loadtype is \"fullload\" :\n",
    "    print(\"loading spark_pax_flight_final table\")\n",
    "    sqlContext.sql(\"create table spark_pax_flight_final as select * from spark_pax_flight\")\n",
    "else:\n",
    "    print(\"Dropping spark_pax_flight_final table\")\n",
    "    sqlContext.sql(\"drop table IF EXISTS spark_pax_flight_final\")\n",
    "    print(\"loading spark_pax_flight_final table\")\n",
    "    sqlContext.sql(\"create table spark_pax_flight_final as select * from spark_pax_flight\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "endtime = time.time()\n",
    "diff = int ( endtime - starttime ) \n",
    "minutes, seconds = diff // 60, diff % 60\n",
    "\n",
    "print(\"time taken: {} mins & {} secs \".format(minutes,seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
