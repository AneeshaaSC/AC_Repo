{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Meals Operation Automation processing future for EK\n",
    "### Combining spark_pax_flight_final with meal option for future flight\n",
    "### Source Data set: \n",
    "###    spark_pax_flight_final with future flight, flight_menu_master, \n",
    "###    menu_itemid_dishcode_master where category = 'Main Course',\n",
    "###    meals_itemid_details_master where dishcategory = 'Main Course' \n",
    "### Target Dataset: spark_pax_flight_future_final\n",
    "### Running Instruction: Define variable RUN_DATE and execute the entire Code in YYYYMMDDHH24MISS\n",
    "###                      loadtype = 'fullload' for first time load\n",
    "###                      loadtype = 'incrementalload' for second time load onwards\n",
    "### The data before and equal RUN_DATE will be treated as History Data and after RUN_DATE + 1 would be treated as Future data\n",
    "\n",
    "### Possible execution time of this code is: fullload time taken: 1 mins & 51 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Importing required python package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json          \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "### This code is for POC purpose. Dont deploy this in production environment without tuning further and \n",
    "### implement exception handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2018-07-27 09:28:53'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "from time import gmtime, strftime\n",
    "strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define Increment Load or Full Load\n",
    "loadtype = 'incrementalload'\n",
    "#loadtype = 'fullload'\n",
    "RUN_DATE = '20180531000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire load will happen upto 2018-05-31 00:00:00\n",
      "and this is incrementalload"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# naive datetime\n",
    "rundate = datetime.datetime.strptime(RUN_DATE, '%Y%m%d%H%M%S')\n",
    "\n",
    "print(\"The entire load will happen upto {}\".format(rundate))\n",
    "print(\"and this is {}\".format(loadtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Create spark session \n",
    "### This steps required to run from jupyter\n",
    "spark = SparkSession.builder.appName(\"Second SparkSession\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# date_format( date_format( t1.flt_schd_dep_date  , 'yyyy-MM-dd HH.mm.ss.S'), 'yyyyMMddHHmmss') \n",
    "### Bringing the dataset for menu for future flight\n",
    "spark_future_menu = spark.sql(\"select t1.flt_carrier_code, t1.flt_number as flight_number, CONCAT(SUBSTR(t1.flt_schd_dep_date,1,4) , SUBSTR(t1.flt_schd_dep_date,6,2) , SUBSTR(t1.flt_schd_dep_date,9,2),SUBSTR(t1.flt_schd_dep_date,12,2),SUBSTR(t1.flt_schd_dep_date,15,2) ) as flight_boarding_time, t1.flt_schd_dep_stn as board_point, t1.cabin_class_code, t1.service_category_code, t1.meal_service_name, t1.meal_service_description, t1.menu_name, t1.meal_type_code, t2.menu_id, t2.item_id , t2.item_code, t2.item_name, t2.category, t2.subcategory, t2.quantity, t2.cmsm_uom_name, t3.dishcode, t3.dishversion, t3.dishcategory, t3.dishsubcategory, t3.cabinclassname, t3.menucardname, t3.dishname, t3.is_m_level_boolean, t3.approved_boolean, t3.ek_sig_dish_boolean, t3.in_use_boolean, t3.cms_status, t3.cms_createdate, t3.effective_from, t3.effective_to, t3.lw_cal_dish_boolean, t3.menu_text_boolean from flight_menu_master t1 inner join ( select * from menu_itemid_dishcode_master where category = 'Main Course' ) as t2 on t1.menu_id = t2.menu_id inner join ( select * from meals_itemid_details_master where dishcategory = 'Main Course' ) as t3 on t2.item_id = t3.item_id where t1.flt_schd_dep_date > ( select from_unixtime(unix_timestamp(max(rundate),'yyyyMMddHHmmss' ) + 86400 ) from loadtable )\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spark_future_menu.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### testing spark_flight data \n",
    "# print(spark_future_menu.count()) ### Testing result 57882 as of 6/14\n",
    "# spark_future_menu.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Bringing the dataset for spark_pax_flight_final\n",
    "spark_pax_flight = spark.sql(\"select flight_number, flight_boarding_time, board_point, id, pax_id, pax_booking_status, destination, cabin_class, booked_class, travelled_class, SeatNumber, pax_type, pax_group_code, boarding_date, upgrade_indicator, ticket_class, handicapped, unaccompanied_minor, priority_pax, check_in_city, date_of_birth, nationality, gender, country_of_birth, country_of_residence,airline_designator,aircraft_type,class_configuration,crew_code,gate_number from spark_pax_flight_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spark_pax_flight.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### testing spark_pax data \n",
    "#print(spark_pax_flight.count())\n",
    "#spark_pax_flight.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark_pax_flight_key  = [ \"flight_number\" , \"flight_boarding_time\" , \"board_point\" ]\n",
    "spark_pax_flight = spark_pax_flight.join(broadcast(spark_future_menu), spark_pax_flight_key , \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### testing spark_pax_flight data \n",
    "#print(spark_pax_flight.count())\n",
    "#spark_pax_flight.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping spark_pax_flight_future_final table\n",
      "DataFrame[]\n",
      "loading spark_pax_flight_future_final table\n",
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "### Loading the pax and flights data into Hive table\n",
    "spark_pax_flight.createOrReplaceTempView(\"spark_pax_flight\")\n",
    "\n",
    "if loadtype is \"fullload\" :\n",
    "    print(\"loading spark_pax_flight_future_final table\")\n",
    "    sqlContext.sql(\"create table spark_pax_flight_future_final as select * from spark_pax_flight\")\n",
    "else:\n",
    "    print(\"Dropping spark_pax_flight_future_final table\")\n",
    "    sqlContext.sql(\"drop table spark_pax_flight_future_final\")\n",
    "    print(\"loading spark_pax_flight_future_final table\")\n",
    "    sqlContext.sql(\"create table spark_pax_flight_future_final as select * from spark_pax_flight\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2018-07-27 09:30:10'"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 1 mins & 17 secs"
     ]
    }
   ],
   "source": [
    "endtime = time.time()\n",
    "diff = int ( endtime - starttime ) \n",
    "minutes, seconds = diff // 60, diff % 60\n",
    "\n",
    "print(\"time taken: {} mins & {} secs \".format(minutes,seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
