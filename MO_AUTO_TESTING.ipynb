{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>316</td><td>application_1529297757126_0323</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-ge-spa.ympkagzigm5elfiwak4c3kqbbb.fx.internal.cloudapp.net:8088/proxy/application_1529297757126_0323/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-ge-spa.ympkagzigm5elfiwak4c3kqbbb.fx.internal.cloudapp.net:30060/node/containerlogs/container_e10_1529297757126_0323_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "### Importing required python package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json          \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "### This code is for POC purpose. Dont deploy this in production environment without tuning further and \n",
    "### implement exception handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2018-06-19 04:52:51'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "from time import gmtime, strftime\n",
    "strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Create spark session \n",
    "### This steps required to run from jupyter\n",
    "spark = SparkSession.builder.appName(\"First SparkSession\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Bringing the dataset for pax\n",
    "spark_pax = spark.sql(\"select pax_id, pax_booking_status, destination, cabin_class, transit_indicator, booked_class, travelled_class, seat_number as SeatNumber, pax_type, pax_group_code, boarding_date, upgrade_indicator, ticket_class, handicapped, unaccompanied_minor, priority_pax, check_in_city, date_of_birth, nationality, gender, country_of_birth, country_of_residence, mfl_id as id from h_macs_pax_joined_all_new where boarding_date >= '2015-01-01 00:00:00.0' and upper(travelled_class) = 'J' UNION select pax_id, pax_booking_status, destination, cabin_class, transit_indicator, booked_class, travelled_class, seat_number as SeatNumber, pax_type, pax_group_code, boarding_date, upgrade_indicator, ticket_class, handicapped, unaccompanied_minor, priority_pax, check_in_city, date_of_birth, nationality, gender, country_of_birth, country_of_residence, mfl_id as id from macs_pax_joined_mvp3 where ( booked_class = 'J' or travelled_class = 'J' ) and boarding_date >= ( select from_unixtime(unix_timestamp(max(rundate),'yyyyMMddHHmmss' )) from loadtable ) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### testing spark_pax data \n",
    "# print(spark_pax.count())  ## 20667814\n",
    "# spark_pax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading spark_analysis_test table\n",
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "### Loading the pax and flights data into Hive table\n",
    "spark_pax.createOrReplaceTempView(\"temptable\")\n",
    "\n",
    "print(\"loading spark_analysis_test table\")\n",
    "sqlContext.sql(\"create table spark_analysis_test as select * from temptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 40 mins & 0 secs"
     ]
    }
   ],
   "source": [
    "endtime = time.time()\n",
    "diff = int ( endtime - starttime ) \n",
    "minutes, seconds = diff // 60, diff % 60\n",
    "\n",
    "print(\"time taken: {} mins & {} secs \".format(minutes,seconds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
