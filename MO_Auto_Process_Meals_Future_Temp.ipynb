{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nfrom datetime import date\nfrom datetime import datetime\nfrom collections import Counter\nfrom numpy import loadtxt"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["\ndef _map_to_pandas(rdds):\n    \"\"\" Needs to be here due to pickling issues \"\"\"\n    return [pd.DataFrame(list(rdds))]\n\ndef toPandas(df, n_partitions=None):\n    \"\"\"\n    Returns the contents of `df` as a local `pandas.DataFrame` in a speedy fashion. The DataFrame is\n    repartitioned if `n_partitions` is passed.\n    :param df:              pyspark.sql.DataFrame\n    :param n_partitions:    int or None\n    :return:                pandas.DataFrame\n    \"\"\"\n    if n_partitions is not None: df = df.repartition(n_partitions)\n    df_pand = df.rdd.mapPartitions(_map_to_pandas).collect()\n    df_pand = pd.concat(df_pand)\n    df_pand.columns = df.columns\n    return df_pand"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["dbutils.widgets.text(\"futuredata\", \"allFutureFlights\", \"Future data\")\n\nfuturedate = dbutils.widgets.get(\"futuredata\")\nprint(futuredate)\nglobal_temp_db = spark.conf.get(\"spark.sql.globalTempDatabase\")\n## Extract pax and flight data from db table: spark_pax_flight_future_final\n\nfuture_pax_data = spark.sql(\"SELECT * from {0}\".format(global_temp_db + \".\" + futuredate))   \n#print(type(spark_future_pax_data)): spark_future_pax_data is a dataframe\nspark_future_pax_data = toPandas(future_pax_data,5)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["\nspark_future_pax_data.rename(columns = {'board_point':'flight_boarding_pt', \n                                        'menu_name': 'menuname'}, inplace = True)\nspark_future_pax_data['flight_boarding_time'] =  spark_future_pax_data['flight_boarding_time'].apply(lambda x : datetime.strptime(x, '%Y%m%d%H%M'))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["spark_future_pax_data.dishsubcategory[spark_future_pax_data.menucardname == 'Cajun chicken'] = 'Poultry'\n\n#M1 = spark_future_pax_data[((spark_future_pax_data.meal_service_name == 'Hot Meal') | ((spark_future_pax_data.flight_number == 306) & (spark_future_pax_data.meal_service_name == 'Hot Breakfast'))) & (spark_future_pax_data.dishcategory == 'Main Course')]\nM1 = spark_future_pax_data[(spark_future_pax_data.meal_service_name == 'Hot Meal') & (spark_future_pax_data.dishcategory == 'Main Course')]\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["cuisine1 = sqlContext.read.format('csv').options(header='true', inferSchema='true', dateFormat='yyyyMMddHHmm').load('/FileStore/tables/Dish_Cuisine_Sandra.csv')\ncuisine = toPandas(cuisine1,5)\ncuisine.Cuisine = cuisine.Cuisine.replace('\\?','',regex=True)\ncuisine.itemname = cuisine.itemname.str.lower()\ncuisine.itemname = cuisine.itemname.str.strip()\n\ncuisine.Cuisine = cuisine.Cuisine.str.lower()\ncuisine.Cuisine = cuisine.Cuisine.str.strip()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["M1.menucardname = M1.menucardname.str.lower()\nM1.menucardname = M1.menucardname.str.strip()\n\nM1 = pd.merge(M1,cuisine,left_on = 'menucardname', right_on = 'itemname', how = 'left')\n\n# Menu cycle, Destination\nM1['menuname'][M1['menuname'] == 'F DXBAUS HM J Q A'] = 'DXBAUS HM J Q A'\nM1['menuname'][M1['menuname'] == 'FEST2017 DXBEUR HMJ'] = 'DXBEUR HMJ FEST2017'\nM1['menuname'][M1['menuname'] == 'FEST2017 DXBGER HMJ'] = 'DXBGER HMJ FEST2017'\nM1['menuname'][M1['menuname'] == 'HO 2017 DXBCDG HM JB'] = 'DXBCDG HM JB'\nM1['menuname'][M1['menuname'] == 'TR DXBMEL HM J T2'] = 'DXBMEL HM J T'\n\nM1['menu_cycle'] = M1['menuname'].str.split().str[-1]\nM1['menu_cycle'][M1['menu_cycle'] == 'JA'] = 'A'\nM1['menu_cycle'][M1['menu_cycle'] == 'JB'] = 'B'\nM1['menu_cycle'][M1['menu_cycle'] == 'FEST17'] = 'FEST2017'\n\nM1['destination'] = M1['menuname'].str.split().str[0].str[3:]\n\n# Age group\nM1['date_of_birth'] = pd.to_datetime(M1['date_of_birth'])\n\nM1['today_date'] = date.today()\nM1['today_date'] = pd.to_datetime(M1['today_date'])\n\nM1['age'] = (M1['today_date']-M1['date_of_birth'])/np.timedelta64(1, 'Y')\n\nbins = [0, 12, 19, 40, 60, 100]\nM1['age_groups'] = pd.cut(M1['age'], bins)\n\nM1['age_group'] = M1['age_groups'].cat.codes\n\nM1['age_group_1'] = np.nan \nM1['age_group_1'][M1['age_group'] == 4] = 'Elders' \nM1['age_group_1'][M1['age_group'] == 3] = 'Middle Aged' \nM1['age_group_1'][M1['age_group'] == 2] = 'Adults' \nM1['age_group_1'][M1['age_group'] == 1] = 'Teenagers'\nM1['age_group_1'][M1['age_group'] == 0] = 'Children'"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["country_codes = sqlContext.read.format('csv').options(header='true', inferSchema='true', dateFormat='yyyyMMddHHmm').load('/FileStore/tables/country_codes_cuisine.csv')\ncountry_codes.createOrReplaceTempView(\"country_codes\")\ncountry_codes = spark.sql(\"select `alpha-2` as alpha_2,country_region as country_region from country_codes\")   #no nulls in columns, only empty strings 249\ncountry_regions = toPandas(country_codes,5)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["M1 = pd.merge(M1,country_regions, left_on = 'nationality', right_on = 'alpha_2', how = 'left')\n\nM1['values'] = 1"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["temp1_dishsub_cuisine = pd.get_dummies(M1[['flight_number', 'flight_boarding_pt', \n                           'flight_boarding_time', 'dishsubcategory','Cuisine']].drop_duplicates(),\ncolumns = ['dishsubcategory','Cuisine'])\n\n#.groupby(['flight_number', 'flight_boarding_pt', \n#                           'flight_boarding_time']).sum().reset_index()\n\n\n# demographics - flight level aggregations\n\npl = ['flight_number', 'flight_boarding_pt', 'flight_boarding_time','destination', \n      'menu_cycle', 'service_category_code']\n\ncolumns_list = ['gender', 'country_region']\n\ntemp1_demographics = pd.pivot_table(M1[['flight_number', 'flight_boarding_pt', \n                           'flight_boarding_time','pax_id','menu_cycle','destination', 'service_category_code',\n                           'age_group_1','gender', 'country_region','values']].drop_duplicates(), \nindex = pl, columns = 'age_group_1',\n                       values = 'values',  \n                       aggfunc=np.sum).reset_index()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["for i in columns_list:\n    temp1_demo = pd.pivot_table(M1[['flight_number', 'flight_boarding_pt', \n                               'flight_boarding_time','pax_id','menu_cycle','destination', 'service_category_code',\n                               'age_group_1','gender', 'country_region','values']].drop_duplicates(), \n    index = pl, columns = i,\n                           values = 'values',  \n                           aggfunc=np.sum).reset_index()\n    temp1_demographics = pd.concat([temp1_demographics, temp1_demo.drop(pl,axis=1)], axis=1)\n    \ntemp1 = pd.merge(temp1_demographics,temp1_dishsub_cuisine, on = ['flight_number', 'flight_boarding_pt', \n                               'flight_boarding_time'])\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# extract basic features from date\ntemp1['year'] = temp1['flight_boarding_time'].dt.year\ntemp1['month'] = temp1['flight_boarding_time'].dt.month\ntemp1['quarter'] = temp1['flight_boarding_time'].dt.quarter\ntemp1['week'] = temp1['flight_boarding_time'].dt.week\ntemp1['day'] = temp1['flight_boarding_time'].dt.day\ntemp1['dayofweek'] = temp1['flight_boarding_time'].dt.dayofweek\n\ntemp1.columns = [w.replace('dishsubcategory_','') for w in temp1.columns]\ntemp1.columns = [w.replace('Cuisine_','') for w in temp1.columns]\n\n# wide to long\ntemp1.rename(columns = {'Poultry':'Meal_Poultry',\n       'Red Meat':'Meal_Red Meat', 'Seafood':'Meal_Seafood', 'Pasta or Vegetarian': 'Meal_Pasta or Vegetarian'}, inplace = True)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["list_melt = temp1.columns.tolist()\nlist_melt = [e for e in list_melt if e not in ('Meal_Others',\n       'Meal_Poultry', 'Meal_Red Meat', 'Meal_Seafood','Meal_Pasta or Vegetarian')]\n\n\ndf1 = (pd.melt(temp1,id_vars = list_melt, value_name='Meal'))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["\n\ndf1[['tmp','cat']] = df1.variable.str.split('_', expand=True)\n\ndf1 = df1.drop(['variable', 'tmp'],axis=1).sort_values(['flight_number', 'flight_boarding_pt', 'flight_boarding_time'])\n\ndf1 = df1[df1.Meal == 1]\n\n\npax_count = M1[['flight_number', 'flight_boarding_pt', \n                           'flight_boarding_time','pax_id']].groupby(['flight_number', 'flight_boarding_pt', \n                           'flight_boarding_time']).agg({'pax_id':'nunique'}).reset_index()\npax_count.columns = ['flight_number', 'flight_boarding_pt', 'flight_boarding_time', 'pax_count']\n#pax_count.pax_count = pax_count.pax_count/3\n\n\ndf1 = pd.merge(df1,pax_count,on=['flight_number', 'flight_boarding_pt', 'flight_boarding_time'])\n\ndf1.rename(columns = {'cat':'dishsubcategory','service_category_code' : 'itemcategory'}, inplace = True)\n\ndf1.itemcategory = df1.itemcategory.replace({'L':'Lunch','D':'Dinner'})"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["spark_df = sqlContext.createDataFrame(df1)\ntableName = \"meals_future_{0}\".format(str(int(datetime.now().timestamp())))\nspark_df.createOrReplaceGlobalTempView(tableName)\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["dbutils.notebook.exit(tableName)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["df1"],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"MO_Auto_Process_Meals_Future_Temp","notebookId":3077481719601021},"nbformat":4,"nbformat_minor":0}
