{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Meals Operation Automation processing for EK\n",
    "### Combining MOD, pax, flight, and meals data set\n",
    "### Source Data set: \n",
    "###              meals_mod_data_mvp3_filtered, spark_pax_flight_final, and meals_menu_dish_master\n",
    "### Target Dataset: ek_meals_ops_mod_final\n",
    "### Running Instruction: Define variable RUN_DATE and execute the entire Code in YYYYMMDDHH24MISS\n",
    "###                      loadtype = 'fullload' for first time load\n",
    "###                      loadtype = 'incrementalload' for second time load onwards\n",
    "### The data before and equal RUN_DATE will be treated as History Data and after RUN_DATE + 1 would be treated as Future data\n",
    "\n",
    "### Possible execution time of this code is: fullload time taken: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>563</td><td>application_1534466702543_2832</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-ge-spa.ympkagzigm5elfiwak4c3kqbbb.fx.internal.cloudapp.net:8088/proxy/application_1534466702543_2832/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-ge-spa.ympkagzigm5elfiwak4c3kqbbb.fx.internal.cloudapp.net:30060/node/containerlogs/container_e15_1534466702543_2832_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "### Importing required python package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json          \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "### This code is for POC purpose. Dont deploy this in production environment without tuning further and \n",
    "### exception handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2018-08-29 12:38:43'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "from time import gmtime, strftime\n",
    "strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Increment Load or Full Load\n",
    "## Precautions for full load. Manually drop spark_pax_flight_final hive table before running this process.\n",
    "loadtype = 'incrementalload'\n",
    "RUN_DATE = '20180630000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire load will happen upto 2018-06-30 00:00:00\n",
      "and this is incrementalload"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# naive datetime\n",
    "rundate = datetime.datetime.strptime(RUN_DATE, '%Y%m%d%H%M%S')\n",
    "\n",
    "print(\"The entire load will happen upto {}\".format(rundate))\n",
    "print(\"and this is {}\".format(loadtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Create spark session \n",
    "### This steps required to run from jupyter\n",
    "spark = SparkSession.builder.appName(\"Mod SparkSession\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>"
     ]
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### creating dataset to bring mod data.\n",
    "### Table Name: meals_mod_data_mvp3_filtered\n",
    "### Data is limited to 100 for testing. Change the query for bringing entire dataset\n",
    "\n",
    "spark_mod = spark.sql(\"SELECT q1.doctype, q1.fltKey, q1.channels, jv.value as value FROM meals_mod_data_mvp3_filtered jv LATERAL VIEW JSON_TUPLE(jv.value, 'DocType','FltKey','channels') q1 AS doctype, fltKey, channels where q1.doctype = 'Order' limit 10\")\n",
    "\n",
    "### Cleaning spark_mod data\n",
    "spark_mod = spark_mod.withColumn('flight_number' , spark_mod.channels.substr(3, 4) ).withColumn('board_point' , spark_mod.channels.substr(8, 3) ).withColumn('flight_boarding_time' , spark_mod.channels.substr(12, 12) )\n",
    "\n",
    "spark_mod = spark_mod.drop(\"channels\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### testing spark_mod data \n",
    "# print(spark_mod.count())   ## 1658086  @ 6/17\n",
    "#spark_mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "type(spark_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsing_json(value):\n",
    "   \n",
    "    NoneType = type(None)\n",
    "    \n",
    "    ### Target output list of list    \n",
    "    keep_col_list_1d = [\"Category\", \"OrderType\",\"SeatNumber\", \"Status\" , \"PaxKey\"]   \n",
    "    \n",
    "    dictdump = json.loads(value) \n",
    "    \n",
    "    ### keep_col_list_1d data preparation\n",
    "    \n",
    "    list_of_1d = [] \n",
    "    \n",
    "    for k in keep_col_list_1d: \n",
    "        if k in dictdump.keys():        \n",
    "            list_of_1d.append(dictdump[k])\n",
    "        else:\n",
    "            list_of_1d.append(np.nan)\n",
    "    \n",
    "    return list_of_1d  \n",
    "    \n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "\n",
    "# parsing_udf = udf(parsing_json, ArrayType(StringType))\n",
    "parsing_udf = udf(parsing_json, ArrayType(StringType()))\n",
    "spark_mod = spark_mod.withColumn(\"new\", parsing_udf(spark_mod.value))\n",
    "\n",
    "spark_mod = spark_mod.withColumn('Category' , spark_mod.new[0] ).withColumn('OrderType' , spark_mod.new[1] ).withColumn('seatnumber' , spark_mod.new[2] ).withColumn('Status' , spark_mod.new[3] ).withColumn('PaxKey' , spark_mod.new[4] )  \n",
    "\n",
    "spark_mod = spark_mod.drop(\"new\" )\n",
    "\n",
    "spark_mod.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsemoddata(value):\n",
    "    \n",
    "    NoneType = type(None)\n",
    "    \n",
    "    ### Target output list of list\n",
    "    items_col_list = ['Category', 'CategoryCode' , 'Code', 'ItemId', 'ItemName', 'ItemOrder', 'MenuId', 'MenuName', 'Status', 'SubCategory' , 'CategoryId' , 'preference']\n",
    "    \n",
    "    dictdump = json.loads(value)    \n",
    "    \n",
    "    ### parsing items\n",
    "    ### items_col_list data preparation\n",
    "    \n",
    "    list_of_item = []     \n",
    "    \n",
    "    for item in dictdump[\"Items\"]:\n",
    "        ineritem = []\n",
    "        for k in items_col_list:\n",
    "            if k in item.keys():\n",
    "                if( isinstance(item[k], NoneType) ):\n",
    "                    ineritem.append(np.nan)\n",
    "                else:\n",
    "                    ineritem.append(item[k])\n",
    "            else:\n",
    "                ineritem.append(np.nan)\n",
    "        \n",
    "        list_of_item.append(ineritem)\n",
    "    \n",
    "    return list_of_item  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parsing_udf = udf(parsing_json, ArrayType(StringType))\n",
    "parsing_udf = udf(parsemoddata , ArrayType(ArrayType(StringType())))\n",
    "spark_mod = spark_mod.withColumn(\"new\", parsing_udf(spark_mod.value))\n",
    "\n",
    "# spark_mod.printSchema() \n",
    "\n",
    "#spark_mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_mod.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "spark_mod = spark_mod.withColumn('new', explode('new'))\n",
    "\n",
    "spark_mod = spark_mod.withColumn('itemCategory' , spark_mod.new[0] ).withColumn('itemCategoryCode' , spark_mod.new[1] ).withColumn('itemcode' , spark_mod.new[2] ).withColumn('ItemId' , spark_mod.new[3] ).withColumn('ItemName' , spark_mod.new[4] ).withColumn('ItemOrder' , spark_mod.new[5] ).withColumn('MenuId' , spark_mod.new[6] ).withColumn('itemMenuName' , spark_mod.new[7] ).withColumn('itemStatus' , spark_mod.new[8] ).withColumn('itemSubCategory' , spark_mod.new[9] ).withColumn('itemCategoryId' , spark_mod.new[10] ).withColumn('itempreference' , spark_mod.new[11] ) \n",
    "\n",
    "spark_mod = spark_mod.drop(\"new\" , \"value\" )\n",
    "\n",
    "# spark_mod.printSchema() \n",
    "\n",
    "# print(spark_mod.count())  ##1834000 \n",
    "spark_mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Bringing the dataset for flights and pax \n",
    "spark_pax_flight = spark.sql(\"select * from spark_pax_flight_final \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### testing spark_flight data \n",
    "# print(spark_pax_flight.count())\n",
    "# spark_pax_flight.show()\n",
    "# spark_pax_flight.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_mod_pax_flight_key = [ \"flight_number\" , \"flight_boarding_time\" , \"board_point\" , \"seatnumber\"]\n",
    "spark_mod_pax_flight = spark_mod.join(spark_pax_flight, spark_mod_pax_flight_key , \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### testing spark_pax_flight data \n",
    "# print(spark_mod_pax_flight.count())\n",
    "# spark_mod_pax_flight.select(\"ItemId\" , \"MenuId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### creating dataset to bring items data.\n",
    "### Table Name: meals_menu_dish_master\n",
    "### Column is limited for testing. Change the query for bringing all the columns.\n",
    "### Recomendation: mention list of columns. \n",
    "\n",
    "spark_items = spark.sql(\"select menuid as ItemId, dishcode, dishversion, dishcategory, dishsubcategory, cabinclassname, menucardname, dishname from meals_menu_dish_master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# date_format( date_format( t1.flt_schd_dep_date  , 'yyyy-MM-dd HH.mm.ss.S'), 'yyyyMMddHHmmss') \n",
    "### Bringing the dataset for menu for history flight\n",
    "# spark_future_menu = spark.sql(\"select t1.flt_carrier_code, t1.flt_number as flight_number, CONCAT(SUBSTR(t1.flt_schd_dep_date,1,4) , SUBSTR(t1.flt_schd_dep_date,6,2) , SUBSTR(t1.flt_schd_dep_date,9,2),SUBSTR(t1.flt_schd_dep_date,12,2),SUBSTR(t1.flt_schd_dep_date,15,2) ) as flight_boarding_time, t1.flt_schd_dep_stn as board_point, t1.cabin_class_code, t1.service_category_code, t1.meal_service_name, t1.meal_service_description, t1.menu_name, t1.meal_type_code, t2.menu_id, t2.item_id as ItemId , t2.item_code, t2.item_name, t2.category, t2.subcategory, t2.quantity, t2.cmsm_uom_name, t3.dishcode, t3.dishversion, t3.dishcategory, t3.dishsubcategory, t3.cabinclassname, t3.menucardname, t3.dishname, t3.is_m_level_boolean, t3.approved_boolean, t3.ek_sig_dish_boolean, t3.in_use_boolean, t3.cms_status, t3.cms_createdate, t3.effective_from, t3.effective_to, t3.lw_cal_dish_boolean, t3.menu_text_boolean from flight_menu_master t1 inner join ( select * from menu_itemid_dishcode_master where category = 'Main Course' ) as t2 on t1.menu_id = t2.menu_id inner join ( select * from meals_itemid_details_master where dishcategory = 'Main Course' ) as t3 on t2.item_id = t3.item_id where t1.flt_schd_dep_date <= ( select from_unixtime(unix_timestamp(max(rundate),'yyyyMMddHHmmss' )) from loadtable )\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "## Add items data in the dataset \n",
    "join_mod_flight_pax_items_key = [ \"ItemId\" ]\n",
    "#join_mod_flight_pax_items_key =  [ \"flight_number\" , \"flight_boarding_time\" , \"board_point\" , \"ItemId\"]\n",
    "join_mod_flight_pax_items = spark_mod_pax_flight.join(broadcast(spark_items), join_mod_flight_pax_items_key , \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(join_mod_flight_pax_items.count())\n",
    "# join_mod_flight_pax_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the pax and flights data into Hive table\n",
    "join_mod_flight_pax_items.createOrReplaceTempView(\"mytempTable\")\n",
    "\n",
    "if loadtype is \"fullload\" :\n",
    "    print(\"loading ek_meals_ops_mod_final table\")\n",
    "    sqlContext.sql(\"create table ek_meals_ops_mod_final as select * from mytempTable\")\n",
    "else:\n",
    "    print(\"Dropping ek_meals_ops_mod_final table\")\n",
    "    sqlContext.sql(\"drop table ek_meals_ops_mod_final\")\n",
    "    print(\"loading ek_meals_ops_mod_final table\")\n",
    "    sqlContext.sql(\"create table ek_meals_ops_mod_final as select * from mytempTable\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "endtime = time.time()\n",
    "diff = int ( endtime - starttime ) \n",
    "minutes, seconds = diff // 60, diff % 60\n",
    "\n",
    "print(\"time taken: {} mins & {} secs \".format(minutes,seconds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
